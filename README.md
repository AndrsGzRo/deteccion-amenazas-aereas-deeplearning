# deteccion-amenazas-aereas-deeplearning
Este proyecto implementa y compara diferentes enfoques de **visi√≥n por computadora** para detectar **amenazas a√©reas** ‚Äîcomo misiles, jets, helic√≥pteros, drones, cohetes y aviones‚Äî utilizando **redes neuronales convolucionales (CNN)**.

El reto consiste en dise√±ar, entrenar y evaluar tres modelos distintos, poniendo en pr√°ctica los principios de **aprendizaje profundo**, **transfer learning** y **validaci√≥n de modelos**.

## Objetivos del proyecto
1. **Preparar la base de datos** y generar los conjuntos de entrenamiento y validaci√≥n.
2. **Dise√±ar una CNN desde cero**, ajustando los hiperpar√°metros.
3. **Implementar una CNN con un modelo preentrenado** (transfer learning).
4. **Generar un modelo en Teachable Machine**.
5. **Validar, evaluar y comparar** los tres modelso en t√©rminos de precisi√≥n.
6. **Exportar los modelos entrenados** para ser utlizados en otros entornos.

## Contexto
En este escenario, la humanidad enfrenta una amenaza sin precedentes: una **plaga zombi** se ha extendido por el mundo.  
Una base cient√≠fica desarrolla la vacuna que podr√≠a detenerla, pero est√° bajo amenaza de ataques a√©reos.

El objetivo consiste en desarrolar un modelo inteligente capaz de **detectar en tiempo real** los objetos voladores, clasific√°ndolos en una de las siguientes seis categor√≠as:
- Misil
- Jet
- Helic√≥ptero
- Dron
- Cohete
- Avi√≥n

## Dataset

El dataset contiene **8,530 im√°genes** en diferentes formatos (`.jpg`, `.jpeg`, `.png`) distribuidas en **6 clases**.  
Para el entrenamiento se utilizaron **6,575 im√°genes** y para validaci√≥n **1,643 im√°genes**.  
Cada imagen fue redimensionada a **224√ó224 p√≠xeles**.

Durante la etapa de preparaci√≥n se aplicaron transformaciones como:

- Reescalado de p√≠xeles (`1/255`)
- *Data Augmentation*: volteo horizontal, rotaci√≥n, zoom y contraste aleatorio
- Separaci√≥n del conjunto de entrenamiento y validaci√≥n

## Modelos Implementados
### CNN desde cero
Modelo dise√±ado con capas convolucionales, normalizaci√≥n por lotes y regularizaci√≥n mediante *Dropout*.  
El entrenamiento se optimiz√≥ con *EarlyStopping* y *ReduceLROnPlateau* para mejorar la eficiencia.

### MobileNetv12
MobileNetv2 es una arquitectura de red neuronal profunda optimizada para dispotivos con recursos limitados. 
Se basa en el uso de **convoluciones separables en profundidad** y un bloque llamado ***inverted residual***, que reduce el n√∫mero de par√°emtros sin perder capacidad de representaci√≥n. 

Ventajas principales:
- Modelo ligero y eficiente, ideal para implementaciones en tiempo real.
- Buena precisi√≥n con bajo costo computacional.
- Excelente opci√≥n cuando se requiere un equilibrio entre velocidad y rendimiento.

### EfficientNetB0 
**EfficientNetB0** es el modelo base de la familia EfficientNet, desarrollada por Google.
Su dise√±o se basa en una b√∫squeda automatizada de arquitectura (NAS) y en un m√©todo de escalado compuesto, que equilibra proporcionalmente la profundidad, anchura y resoluci√≥n de la red.

Ventajas principales: 
-Alta precisi√≥n con menos par√°metros comparado con otros modelos grandes.
-Excelente eficiencia computacional: logra mejores resultados con menor tama√±o y tiempo de entrenamiento.
- Escalable a variantes m√°s potentes (B1‚ÄìB7) seg√∫n los recursos disponibles.
### Teachable Machine
Modelo generado mediante la herramienta **[Teachable Machine de Google](https://teachablemachine.withgoogle.com/)**, exportado y probado en Google Colab.

## Comparaci√≥n de Modelos:

| Modelo | Tipo de entrenamiento | P√©rdida (Loss) | Exactitud (Accuracy) |
|:--|:--|:--:|:--:|
| CNN (desde cero) | Entrenamiento desde cero | **1.5589** | **0.3889** |
| MobileNetV2 | Pre-entrenado + Fine-tuning | **0.9856** | **0.6324** |
| EfficientNetB0 | Pre-entrenado + Fine-tuning | **0.8807** | **0.6726** |
| Teachable Machine | Modelo externo (transfer) | **2.9628** | **0.5326** |


## Modelo Seleccionado üèÜ
El modelo final elegido fue **EfficientNetB0** con Fine-Tuning, presenta:
- **Mayor exactitud** (0.6726)
- **Menor p√©rdida** (0.8807)
- Buen balance entre rendimiento y eficiencia computacional
- 
## Tecnolog√≠as utilizads 
- **Python 3.10+**
- **TensorFlow / Keras**
- **NumPy**, **Matplotlib**, **Pandas**
- **Google Colab**
- **Teachable Machine**
- **Scikit-learn**

## Conclusiones
- El uso de **modelos preentrenados y fine-tuning** permiti√≥ obtener una mejora significativa frente a una CNN entrenada desde cero.
- **EfficientNetB0** logr√≥ el mejor rendimiento, mostrando una excelente capacidad de generalizaci√≥n y eficiencia.
- La aplicaci√≥n de **Data Augmentation y callbacks de regularizaci√≥n** contribuy√≥ a un entrenamiento m√°s estable y a evitar el sobreajuste.  
